{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d589c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bb82bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 7427877.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 28883093.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 8420110.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4541246.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('.', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6644b526",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa9klEQVR4nO3df3DU953f8deaH2vgVnunYmlXQVZUB2oPoqQBwo/DIGhQ0Y0ZY5wctm8ykCYe/xDcUOH6gukUXSaHfOTMkIts0nhyGCYQmNxgTAtnrBxI2INxZQ7HlLhEPkRQDskqstkVMl6Q+PQPytYLWOSz3uWtlZ6PmZ1Bu9833w9ff+2nv+zqq4BzzgkAAAO3WS8AADB4ESEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGBmqPUCrnX58mWdOXNGoVBIgUDAejkAAE/OOXV1damoqEi33db3tU6/i9CZM2dUXFxsvQwAwOfU2tqqMWPG9LlNv4tQKBSSJM3Un2iohhmvBgDgq0eX9Ib2Jv973pesReiFF17QD37wA7W1tWn8+PHasGGD7r333pvOXf0ruKEapqEBIgQAOef/3ZH093lLJSsfTNixY4dWrFih1atX6+jRo7r33ntVWVmp06dPZ2N3AIAclZUIrV+/Xt/+9rf1ne98R/fcc482bNig4uJibdy4MRu7AwDkqIxH6OLFizpy5IgqKipSnq+oqNChQ4eu2z6RSCgej6c8AACDQ8YjdPbsWfX29qqwsDDl+cLCQrW3t1+3fW1trcLhcPLBJ+MAYPDI2jerXvuGlHPuhm9SrVq1SrFYLPlobW3N1pIAAP1Mxj8dN3r0aA0ZMuS6q56Ojo7rro4kKRgMKhgMZnoZAIAckPEroeHDh2vSpEmqr69Peb6+vl4zZszI9O4AADksK98nVF1drW9+85uaPHmypk+frp/85Cc6ffq0Hn/88WzsDgCQo7ISocWLF6uzs1Pf+9731NbWprKyMu3du1clJSXZ2B0AIEcFnHPOehGfFo/HFQ6HVa77uWMCAOSgHndJDXpFsVhMeXl5fW7Lj3IAAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzAy1XgDQnwSG+v8rMeSO0VlYSWaceOqLac31jrzsPVNyV4f3zMgnA94z7euHe8/80+Qd3jOSdLa323tm6i9Wes98qfqw98xAwZUQAMAMEQIAmMl4hGpqahQIBFIekUgk07sBAAwAWXlPaPz48frlL3+Z/HrIkCHZ2A0AIMdlJUJDhw7l6gcAcFNZeU+oublZRUVFKi0t1UMPPaSTJ09+5raJRELxeDzlAQAYHDIeoalTp2rLli3at2+fXnzxRbW3t2vGjBnq7Oy84fa1tbUKh8PJR3FxcaaXBADopzIeocrKSj344IOaMGGCvva1r2nPnj2SpM2bN99w+1WrVikWiyUfra2tmV4SAKCfyvo3q44aNUoTJkxQc3PzDV8PBoMKBoPZXgYAoB/K+vcJJRIJvffee4pGo9neFQAgx2Q8Qk899ZQaGxvV0tKit956S1//+tcVj8e1ZMmSTO8KAJDjMv7Xcb/73e/08MMP6+zZs7rjjjs0bdo0HT58WCUlJZneFQAgx2U8Qtu3b8/0b4l+asg9Y71nXHCY98yZ2X/oPXNhmv+NJyUpP+w/9/rE9G6OOdD8w8ch75m/rpvvPfPWhG3eMy2XLnjPSNKzH8zznil63aW1r8GKe8cBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGay/kPt0P/1ln8lrbn1Lz3vPTNu2PC09oVb65Lr9Z75rz9a6j0ztNv/Zp/Tf7HMeyb0Lz3eM5IUPOt/49ORb7+V1r4GK66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIa7aEPBE2fSmjvySbH3zLhhH6S1r4FmZds075mT50d7z7x01997z0hS7LL/3a0L//ZQWvvqz/yPAnxxJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGplBPW3tacz/66294z/zV/G7vmSHv/oH3zK+e/JH3TLq+f/bfes+8/7WR3jO959q8Zx6Z/qT3jCSd+nP/mVL9Kq19YXDjSggAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMNTJG2/E1ves/c8d//lfdMb+eH3jPjy/6j94wkHZ/1d94zu38y23um4Nwh75l0BN5M76aipf7/aIG0cCUEADBDhAAAZrwjdPDgQS1YsEBFRUUKBALatWtXyuvOOdXU1KioqEgjRoxQeXm5jh8/nqn1AgAGEO8IdXd3a+LEiaqrq7vh6+vWrdP69etVV1enpqYmRSIRzZs3T11dXZ97sQCAgcX7gwmVlZWqrKy84WvOOW3YsEGrV6/WokWLJEmbN29WYWGhtm3bpscee+zzrRYAMKBk9D2hlpYWtbe3q6KiIvlcMBjU7NmzdejQjT8NlEgkFI/HUx4AgMEhoxFqb2+XJBUWFqY8X1hYmHztWrW1tQqHw8lHcXFxJpcEAOjHsvLpuEAgkPK1c+66565atWqVYrFY8tHa2pqNJQEA+qGMfrNqJBKRdOWKKBqNJp/v6Oi47uroqmAwqGAwmMllAAByREavhEpLSxWJRFRfX5987uLFi2psbNSMGTMyuSsAwADgfSV0/vx5vf/++8mvW1pa9M477yg/P1933nmnVqxYobVr12rs2LEaO3as1q5dq5EjR+qRRx7J6MIBALnPO0Jvv/225syZk/y6urpakrRkyRK99NJLevrpp3XhwgU9+eST+uijjzR16lS99tprCoVCmVs1AGBACDjnnPUiPi0ejyscDqtc92toYJj1cpCjfvPfpqQ3d9+PvWe+9dt/7z3zf2am8c3bl3v9ZwADPe6SGvSKYrGY8vLy+tyWe8cBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADATEZ/sirQX9zzF79Ja+5bE/zviL2p5B+9Z2Z/o8p7JrTjsPcM0N9xJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGphiQes/F0prrfOIe75nTuy94z3z3+1u8Z1b96QPeM+5o2HtGkor/6k3/IefS2hcGN66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAU+JTLv3rPe+ahv/zP3jNb1/yN98w70/xveqpp/iOSNH7UMu+ZsS+2ec/0nDzlPYOBhSshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMBMwDnnrBfxafF4XOFwWOW6X0MDw6yXA2SF++Mve8/kPfs775mf/+t93jPpuvvAd7xn/s1fxrxneptPes/g1upxl9SgVxSLxZSXl9fntlwJAQDMECEAgBnvCB08eFALFixQUVGRAoGAdu3alfL60qVLFQgEUh7TpqX5Q00AAAOad4S6u7s1ceJE1dXVfeY28+fPV1tbW/Kxd+/ez7VIAMDA5P2TVSsrK1VZWdnnNsFgUJFIJO1FAQAGh6y8J9TQ0KCCggKNGzdOjz76qDo6Oj5z20QioXg8nvIAAAwOGY9QZWWltm7dqv379+u5555TU1OT5s6dq0QiccPta2trFQ6Hk4/i4uJMLwkA0E95/3XczSxevDj567KyMk2ePFklJSXas2ePFi1adN32q1atUnV1dfLreDxOiABgkMh4hK4VjUZVUlKi5ubmG74eDAYVDAazvQwAQD+U9e8T6uzsVGtrq6LRaLZ3BQDIMd5XQufPn9f777+f/LqlpUXvvPOO8vPzlZ+fr5qaGj344IOKRqM6deqUnnnmGY0ePVoPPPBARhcOAMh93hF6++23NWfOnOTXV9/PWbJkiTZu3Khjx45py5YtOnfunKLRqObMmaMdO3YoFAplbtUAgAGBG5gCOWJIYYH3zJnFX0prX2/9xQ+9Z25L42/3/6ylwnsmNrPTewa3FjcwBQDkBCIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjJ+k9WBZAZvR90eM8U/q3/jCR98nSP98zIwHDvmRe/+D+8Z+57YIX3zMiX3/Kewa3BlRAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYbmAIGLs/8svfMP3/jdu+Zsi+f8p6R0rsZaTp+9OG/854Z+crbWVgJrHAlBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamwKcEJpd5z/zmz/1v9vniH2/2npl1+0XvmVsp4S55zxz+sNR/R5fb/GfQb3ElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4Qam6PeGlpZ4z/zzt4rS2lfN4u3eMw/+wdm09tWfPfPBZO+Zxh9O8575o81ves9gYOFKCABghggBAMx4Rai2tlZTpkxRKBRSQUGBFi5cqBMnTqRs45xTTU2NioqKNGLECJWXl+v48eMZXTQAYGDwilBjY6Oqqqp0+PBh1dfXq6enRxUVFeru7k5us27dOq1fv151dXVqampSJBLRvHnz1NXVlfHFAwBym9cHE1599dWUrzdt2qSCggIdOXJEs2bNknNOGzZs0OrVq7Vo0SJJ0ubNm1VYWKht27bpsccey9zKAQA573O9JxSLxSRJ+fn5kqSWlha1t7eroqIiuU0wGNTs2bN16NChG/4eiURC8Xg85QEAGBzSjpBzTtXV1Zo5c6bKysokSe3t7ZKkwsLClG0LCwuTr12rtrZW4XA4+SguLk53SQCAHJN2hJYtW6Z3331XP//5z697LRAIpHztnLvuuatWrVqlWCyWfLS2tqa7JABAjknrm1WXL1+u3bt36+DBgxozZkzy+UgkIunKFVE0Gk0+39HRcd3V0VXBYFDBYDCdZQAAcpzXlZBzTsuWLdPOnTu1f/9+lZaWprxeWlqqSCSi+vr65HMXL15UY2OjZsyYkZkVAwAGDK8roaqqKm3btk2vvPKKQqFQ8n2ecDisESNGKBAIaMWKFVq7dq3Gjh2rsWPHau3atRo5cqQeeeSRrPwBAAC5yytCGzdulCSVl5enPL9p0yYtXbpUkvT000/rwoULevLJJ/XRRx9p6tSpeu211xQKhTKyYADAwBFwzjnrRXxaPB5XOBxWue7X0MAw6+WgD0O/eKf3TGxS9OYbXWPx9169+UbXePwPT3rP9Hcr2/xvEPrmC/43IpWk/Jf+p//Q5d609oWBp8ddUoNeUSwWU15eXp/bcu84AIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmEnrJ6ui/xoajXjPfPh3o9La1xOljd4zD4c+SGtf/dmyf5npPfNPG7/sPTP67/+X90x+15veM8CtxJUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5jeIhf/w2T/mf/0offMM1/a6z1TMaLbe6a/+6D3Qlpzs3av9J65+7/8b++Z/HP+Nxa97D0B9H9cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZriB6S1yaqF/738z4RdZWEnmPH/uLu+ZHzZWeM8EegPeM3d/v8V7RpLGfvCW90xvWnsCIHElBAAwRIQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYCTjnnPUiPi0ejyscDqtc92toYJj1cgAAnnrcJTXoFcViMeXl5fW5LVdCAAAzRAgAYMYrQrW1tZoyZYpCoZAKCgq0cOFCnThxImWbpUuXKhAIpDymTZuW0UUDAAYGrwg1NjaqqqpKhw8fVn19vXp6elRRUaHu7u6U7ebPn6+2trbkY+/evRldNABgYPD6yaqvvvpqytebNm1SQUGBjhw5olmzZiWfDwaDikQimVkhAGDA+lzvCcViMUlSfn5+yvMNDQ0qKCjQuHHj9Oijj6qjo+Mzf49EIqF4PJ7yAAAMDmlHyDmn6upqzZw5U2VlZcnnKysrtXXrVu3fv1/PPfecmpqaNHfuXCUSiRv+PrW1tQqHw8lHcXFxuksCAOSYtL9PqKqqSnv27NEbb7yhMWPGfOZ2bW1tKikp0fbt27Vo0aLrXk8kEimBisfjKi4u5vuEACBH+XyfkNd7QlctX75cu3fv1sGDB/sMkCRFo1GVlJSoubn5hq8Hg0EFg8F0lgEAyHFeEXLOafny5Xr55ZfV0NCg0tLSm850dnaqtbVV0Wg07UUCAAYmr/eEqqqq9LOf/Uzbtm1TKBRSe3u72tvbdeHCBUnS+fPn9dRTT+nNN9/UqVOn1NDQoAULFmj06NF64IEHsvIHAADkLq8roY0bN0qSysvLU57ftGmTli5dqiFDhujYsWPasmWLzp07p2g0qjlz5mjHjh0KhUIZWzQAYGDw/uu4vowYMUL79u37XAsCAAwe3DsOAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGBmqPUCruWckyT16JLkjBcDAPDWo0uS/v9/z/vS7yLU1dUlSXpDe41XAgD4PLq6uhQOh/vcJuB+n1TdQpcvX9aZM2cUCoUUCARSXovH4youLlZra6vy8vKMVmiP43AFx+EKjsMVHIcr+sNxcM6pq6tLRUVFuu22vt/16XdXQrfddpvGjBnT5zZ5eXmD+iS7iuNwBcfhCo7DFRyHK6yPw82ugK7igwkAADNECABgJqciFAwGtWbNGgWDQeulmOI4XMFxuILjcAXH4YpcOw797oMJAIDBI6euhAAAAwsRAgCYIUIAADNECABgJqci9MILL6i0tFS33367Jk2apNdff916SbdUTU2NAoFAyiMSiVgvK+sOHjyoBQsWqKioSIFAQLt27Up53TmnmpoaFRUVacSIESovL9fx48dtFptFNzsOS5cuve78mDZtms1is6S2tlZTpkxRKBRSQUGBFi5cqBMnTqRsMxjOh9/nOOTK+ZAzEdqxY4dWrFih1atX6+jRo7r33ntVWVmp06dPWy/tlho/frza2tqSj2PHjlkvKeu6u7s1ceJE1dXV3fD1devWaf369aqrq1NTU5MikYjmzZuXvA/hQHGz4yBJ8+fPTzk/9u4dWPdgbGxsVFVVlQ4fPqz6+nr19PSooqJC3d3dyW0Gw/nw+xwHKUfOB5cjvvrVr7rHH3885bm7777bffe73zVa0a23Zs0aN3HiROtlmJLkXn755eTXly9fdpFIxD377LPJ5z755BMXDofdj3/8Y4MV3hrXHgfnnFuyZIm7//77TdZjpaOjw0lyjY2NzrnBez5cexycy53zISeuhC5evKgjR46ooqIi5fmKigodOnTIaFU2mpubVVRUpNLSUj300EM6efKk9ZJMtbS0qL29PeXcCAaDmj179qA7NySpoaFBBQUFGjdunB599FF1dHRYLymrYrGYJCk/P1/S4D0frj0OV+XC+ZATETp79qx6e3tVWFiY8nxhYaHa29uNVnXrTZ06VVu2bNG+ffv04osvqr29XTNmzFBnZ6f10sxc/ec/2M8NSaqsrNTWrVu1f/9+Pffcc2pqatLcuXOVSCSsl5YVzjlVV1dr5syZKisrkzQ4z4cbHQcpd86HfncX7b5c+6MdnHPXPTeQVVZWJn89YcIETZ8+XXfddZc2b96s6upqw5XZG+znhiQtXrw4+euysjJNnjxZJSUl2rNnjxYtWmS4suxYtmyZ3n33Xb3xxhvXvTaYzofPOg65cj7kxJXQ6NGjNWTIkOv+T6ajo+O6/+MZTEaNGqUJEyaoubnZeilmrn46kHPjetFoVCUlJQPy/Fi+fLl2796tAwcOpPzol8F2PnzWcbiR/no+5ESEhg8frkmTJqm+vj7l+fr6es2YMcNoVfYSiYTee+89RaNR66WYKS0tVSQSSTk3Ll68qMbGxkF9bkhSZ2enWltbB9T54ZzTsmXLtHPnTu3fv1+lpaUprw+W8+Fmx+FG+u35YPihCC/bt293w4YNcz/96U/dr3/9a7dixQo3atQod+rUKeul3TIrV650DQ0N7uTJk+7w4cPuvvvuc6FQaMAfg66uLnf06FF39OhRJ8mtX7/eHT161P32t791zjn37LPPunA47Hbu3OmOHTvmHn74YReNRl08HjdeeWb1dRy6urrcypUr3aFDh1xLS4s7cOCAmz59uvvCF74woI7DE0884cLhsGtoaHBtbW3Jx8cff5zcZjCcDzc7Drl0PuRMhJxz7vnnn3clJSVu+PDh7itf+UrKxxEHg8WLF7toNOqGDRvmioqK3KJFi9zx48etl5V1Bw4ccJKueyxZssQ5d+VjuWvWrHGRSMQFg0E3a9Ysd+zYMdtFZ0Ffx+Hjjz92FRUV7o477nDDhg1zd955p1uyZIk7ffq09bIz6kZ/fklu06ZNyW0Gw/lws+OQS+cDP8oBAGAmJ94TAgAMTEQIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmf8Lw4IYymq+HboAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dataset.data[0].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12610088",
   "metadata": {},
   "source": [
    "## Задача 1. Обучить полносвязную модель на MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c920d7b",
   "metadata": {},
   "source": [
    "### Реализуем модель в стандартном виде, в котором она была представлена в лекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a04fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.linear1(x)))\n",
    "        x = self.do2(self.activation(self.linear2(x)))\n",
    "\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c6573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data: list):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "        pics.append(numpy.array(item[0]))\n",
    "        target.append(item[1])\n",
    "    pics = torch.from_numpy(numpy.array(pics)).float() / 255 # B x W x H\n",
    "    target = torch.from_numpy(numpy.array(target))\n",
    "\n",
    "    return {\n",
    "        'data': pics.view(pics.size(0), -1),\n",
    "        'target': target,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca54dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "inp_dim = 28 * 28\n",
    "hidden = 256\n",
    "out_dim = 10\n",
    "device_id = -1\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87e9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(inp_dim, hidden, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b3d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.305626153945923\n",
      "epoch: 0, step: 200, loss: 0.348014771938324\n",
      "epoch: 0, step: 400, loss: 0.1633419394493103\n",
      "epoch: 1, step: 0, loss: 0.2061845362186432\n",
      "epoch: 1, step: 200, loss: 0.1847580373287201\n",
      "epoch: 1, step: 400, loss: 0.1580585390329361\n",
      "epoch: 2, step: 0, loss: 0.09520301967859268\n",
      "epoch: 2, step: 200, loss: 0.06977543979883194\n",
      "epoch: 2, step: 400, loss: 0.02403486892580986\n",
      "epoch: 3, step: 0, loss: 0.05281877517700195\n",
      "epoch: 3, step: 200, loss: 0.023784678429365158\n",
      "epoch: 3, step: 400, loss: 0.08850326389074326\n",
      "epoch: 4, step: 0, loss: 0.04691809415817261\n",
      "epoch: 4, step: 200, loss: 0.030766045674681664\n",
      "epoch: 4, step: 400, loss: 0.061828285455703735\n",
      "epoch: 5, step: 0, loss: 0.024476803839206696\n",
      "epoch: 5, step: 200, loss: 0.07682830095291138\n",
      "epoch: 5, step: 400, loss: 0.07606303691864014\n",
      "epoch: 6, step: 0, loss: 0.014368685893714428\n",
      "epoch: 6, step: 200, loss: 0.04329071566462517\n",
      "epoch: 6, step: 400, loss: 0.0518050417304039\n",
      "epoch: 7, step: 0, loss: 0.016529805958271027\n",
      "epoch: 7, step: 200, loss: 0.04356101527810097\n",
      "epoch: 7, step: 400, loss: 0.09299492835998535\n",
      "epoch: 8, step: 0, loss: 0.04557810351252556\n",
      "epoch: 8, step: 200, loss: 0.01714162714779377\n",
      "epoch: 8, step: 400, loss: 0.05096706748008728\n",
      "epoch: 9, step: 0, loss: 0.0630933940410614\n",
      "epoch: 9, step: 200, loss: 0.04213205724954605\n",
      "epoch: 9, step: 400, loss: 0.028987454250454903\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device).long())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "        torch.save(model.state_dict(), f'./chkpt_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454759f",
   "metadata": {},
   "source": [
    "### Попробуем применить Сигмоиду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cde33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.linear1(x)))\n",
    "        x = self.do2(self.activation(self.linear2(x)))\n",
    "\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90a088ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(inp_dim, hidden, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb78bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.329514503479004\n",
      "epoch: 0, step: 200, loss: 0.43864870071411133\n",
      "epoch: 0, step: 400, loss: 0.26574480533599854\n",
      "epoch: 1, step: 0, loss: 0.2520867586135864\n",
      "epoch: 1, step: 200, loss: 0.26213762164115906\n",
      "epoch: 1, step: 400, loss: 0.12980355322360992\n",
      "epoch: 2, step: 0, loss: 0.198484867811203\n",
      "epoch: 2, step: 200, loss: 0.30414462089538574\n",
      "epoch: 2, step: 400, loss: 0.18149374425411224\n",
      "epoch: 3, step: 0, loss: 0.1180005595088005\n",
      "epoch: 3, step: 200, loss: 0.19922061264514923\n",
      "epoch: 3, step: 400, loss: 0.08821189403533936\n",
      "epoch: 4, step: 0, loss: 0.09459053725004196\n",
      "epoch: 4, step: 200, loss: 0.11067195981740952\n",
      "epoch: 4, step: 400, loss: 0.07571357488632202\n",
      "epoch: 5, step: 0, loss: 0.11167462915182114\n",
      "epoch: 5, step: 200, loss: 0.1123131513595581\n",
      "epoch: 5, step: 400, loss: 0.15625999867916107\n",
      "epoch: 6, step: 0, loss: 0.0767073929309845\n",
      "epoch: 6, step: 200, loss: 0.20082297921180725\n",
      "epoch: 6, step: 400, loss: 0.09358647465705872\n",
      "epoch: 7, step: 0, loss: 0.08907385915517807\n",
      "epoch: 7, step: 200, loss: 0.1011919304728508\n",
      "epoch: 7, step: 400, loss: 0.10360103845596313\n",
      "epoch: 8, step: 0, loss: 0.05843836069107056\n",
      "epoch: 8, step: 200, loss: 0.045170728117227554\n",
      "epoch: 8, step: 400, loss: 0.02601415477693081\n",
      "epoch: 9, step: 0, loss: 0.031224792823195457\n",
      "epoch: 9, step: 200, loss: 0.07157334685325623\n",
      "epoch: 9, step: 400, loss: 0.10439272224903107\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device).long())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "        torch.save(model.state_dict(), f'./chkpt_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сигмоида отработала хуже"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a28849",
   "metadata": {},
   "source": [
    "### Добавим слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53fcbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.do3 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.linear1(x)))\n",
    "        x = self.do2(self.activation(self.linear2(x)))\n",
    "        x = self.do3(self.activation(self.linear3(x)))\n",
    "\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94fa9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(inp_dim, hidden, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4077578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.3118743896484375\n",
      "epoch: 0, step: 200, loss: 0.2612638473510742\n",
      "epoch: 0, step: 400, loss: 0.15483027696609497\n",
      "epoch: 1, step: 0, loss: 0.14875347912311554\n",
      "epoch: 1, step: 200, loss: 0.06868734210729599\n",
      "epoch: 1, step: 400, loss: 0.13277553021907806\n",
      "epoch: 2, step: 0, loss: 0.14985966682434082\n",
      "epoch: 2, step: 200, loss: 0.05571310967206955\n",
      "epoch: 2, step: 400, loss: 0.02602122537791729\n",
      "epoch: 3, step: 0, loss: 0.025548867881298065\n",
      "epoch: 3, step: 200, loss: 0.09426605701446533\n",
      "epoch: 3, step: 400, loss: 0.03576326742768288\n",
      "epoch: 4, step: 0, loss: 0.012749879620969296\n",
      "epoch: 4, step: 200, loss: 0.06389794498682022\n",
      "epoch: 4, step: 400, loss: 0.07100056856870651\n",
      "epoch: 5, step: 0, loss: 0.037895962595939636\n",
      "epoch: 5, step: 200, loss: 0.04375274479389191\n",
      "epoch: 5, step: 400, loss: 0.014639684930443764\n",
      "epoch: 6, step: 0, loss: 0.0786169171333313\n",
      "epoch: 6, step: 200, loss: 0.02240810915827751\n",
      "epoch: 6, step: 400, loss: 0.07619648426771164\n",
      "epoch: 7, step: 0, loss: 0.03327225148677826\n",
      "epoch: 7, step: 200, loss: 0.03123701922595501\n",
      "epoch: 7, step: 400, loss: 0.053478777408599854\n",
      "epoch: 8, step: 0, loss: 0.006929412484169006\n",
      "epoch: 8, step: 200, loss: 0.028715232387185097\n",
      "epoch: 8, step: 400, loss: 0.04212295264005661\n",
      "epoch: 9, step: 0, loss: 0.010950465686619282\n",
      "epoch: 9, step: 200, loss: 0.019796013832092285\n",
      "epoch: 9, step: 400, loss: 0.05754368007183075\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device).long())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "        torch.save(model.state_dict(), f'./chkpt_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Результат получше, чем в лекции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16b4b9",
   "metadata": {},
   "source": [
    "### Попробуем между слоями применить разные активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10675ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.do3 = nn.Dropout(dropout_p)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation1(self.linear1(x)))\n",
    "        x = self.do2(self.activation2(self.linear2(x)))\n",
    "        x = self.do3(self.activation1(self.linear3(x)))\n",
    "\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7326c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(inp_dim, hidden, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92efcb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.297086238861084\n",
      "epoch: 0, step: 200, loss: 0.31946372985839844\n",
      "epoch: 0, step: 400, loss: 0.2344718724489212\n",
      "epoch: 1, step: 0, loss: 0.2560077905654907\n",
      "epoch: 1, step: 200, loss: 0.1164950504899025\n",
      "epoch: 1, step: 400, loss: 0.134099081158638\n",
      "epoch: 2, step: 0, loss: 0.055020302534103394\n",
      "epoch: 2, step: 200, loss: 0.13470521569252014\n",
      "epoch: 2, step: 400, loss: 0.08118443191051483\n",
      "epoch: 3, step: 0, loss: 0.08790833503007889\n",
      "epoch: 3, step: 200, loss: 0.10459334403276443\n",
      "epoch: 3, step: 400, loss: 0.15526628494262695\n",
      "epoch: 4, step: 0, loss: 0.046109702438116074\n",
      "epoch: 4, step: 200, loss: 0.05787033587694168\n",
      "epoch: 4, step: 400, loss: 0.058076608926057816\n",
      "epoch: 5, step: 0, loss: 0.05488448962569237\n",
      "epoch: 5, step: 200, loss: 0.0370866023004055\n",
      "epoch: 5, step: 400, loss: 0.03335827589035034\n",
      "epoch: 6, step: 0, loss: 0.06112835928797722\n",
      "epoch: 6, step: 200, loss: 0.017222633585333824\n",
      "epoch: 6, step: 400, loss: 0.020178690552711487\n",
      "epoch: 7, step: 0, loss: 0.013453844003379345\n",
      "epoch: 7, step: 200, loss: 0.07429499179124832\n",
      "epoch: 7, step: 400, loss: 0.10801082104444504\n",
      "epoch: 8, step: 0, loss: 0.0423930399119854\n",
      "epoch: 8, step: 200, loss: 0.011998245492577553\n",
      "epoch: 8, step: 400, loss: 0.03221939504146576\n",
      "epoch: 9, step: 0, loss: 0.03912191838026047\n",
      "epoch: 9, step: 200, loss: 0.03246849402785301\n",
      "epoch: 9, step: 400, loss: 0.007211718242615461\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device).long())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "        torch.save(model.state_dict(), f'./chkpt_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Неоднозначный результат, на первых эпохах результат заметно хуже, однако в конце loss заметно ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7f7a1",
   "metadata": {},
   "source": [
    "## Задача 2. Обучить глубокую сверточную сеть на MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0619652",
   "metadata": {},
   "source": [
    "### Реализуем модель в стандартном виде, в котором она была представлена в лекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fef29df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_ch, hidden_ch, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, hidden_ch, \n",
    "                               kernel_size=5, padding=2, stride=2) # уменьшаем картинку в 2 раза\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, 2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.linear3 = nn.Linear(2 * 14 * 14, output_dim)\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.bn1(self.conv1(x))))\n",
    "        x = self.do2(self.activation(self.bn2(self.conv2(x))))\n",
    "        x = self.activation(self.bn3(self.conv3(x))) # B x 2 x 14 x 14\n",
    "        \n",
    "        return self.linear3(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef21249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "input_ch = 1\n",
    "hidden_ch = 128\n",
    "out_dim = 10\n",
    "device_id = -1\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88396601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_conv(data: list):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "        pics.append(numpy.array(item[0])) # W x H\n",
    "        target.append(item[1])\n",
    "    pics = torch.from_numpy(numpy.array(pics)).float() / 255 # B x W x H\n",
    "    target = torch.from_numpy(numpy.array(target))\n",
    "        \n",
    "    return {\n",
    "        'data': pics.unsqueeze(1), # B x 1(C) x W x H\n",
    "        'target': target.long(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c41492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = ConvModel(input_ch, hidden_ch, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3d165c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.38936448097229\n",
      "epoch: 0, step: 200, loss: 0.20370446145534515\n",
      "epoch: 0, step: 400, loss: 0.16710509359836578\n",
      "epoch: 1, step: 0, loss: 0.19182851910591125\n",
      "epoch: 1, step: 200, loss: 0.04412289336323738\n",
      "epoch: 1, step: 400, loss: 0.06302698701620102\n",
      "epoch: 2, step: 0, loss: 0.10270051658153534\n",
      "epoch: 2, step: 200, loss: 0.08635444939136505\n",
      "epoch: 2, step: 400, loss: 0.02407689206302166\n",
      "epoch: 3, step: 0, loss: 0.03947454318404198\n",
      "epoch: 3, step: 200, loss: 0.04801643639802933\n",
      "epoch: 3, step: 400, loss: 0.04819931834936142\n",
      "epoch: 4, step: 0, loss: 0.018404874950647354\n",
      "epoch: 4, step: 200, loss: 0.061334628611803055\n",
      "epoch: 4, step: 400, loss: 0.01633421890437603\n",
      "epoch: 5, step: 0, loss: 0.013739900663495064\n",
      "epoch: 5, step: 200, loss: 0.013064589351415634\n",
      "epoch: 5, step: 400, loss: 0.00803206767886877\n",
      "epoch: 6, step: 0, loss: 0.025635648518800735\n",
      "epoch: 6, step: 200, loss: 0.007479890249669552\n",
      "epoch: 6, step: 400, loss: 0.025852583348751068\n",
      "epoch: 7, step: 0, loss: 0.018240641802549362\n",
      "epoch: 7, step: 200, loss: 0.011437288485467434\n",
      "epoch: 7, step: 400, loss: 0.022281354293227196\n",
      "epoch: 8, step: 0, loss: 0.06243254244327545\n",
      "epoch: 8, step: 200, loss: 0.052240096032619476\n",
      "epoch: 8, step: 400, loss: 0.058679673820734024\n",
      "epoch: 9, step: 0, loss: 0.01221725344657898\n",
      "epoch: 9, step: 200, loss: 0.0012384271249175072\n",
      "epoch: 9, step: 400, loss: 0.00870505254715681\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn_conv,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model_conv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    torch.save(model_conv.state_dict(), f'./chkpt_conv_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21c967",
   "metadata": {},
   "source": [
    "### Попробуем поменять активацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac73aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_ch, hidden_ch, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, hidden_ch, \n",
    "                               kernel_size=5, padding=2, stride=2) # уменьшаем картинку в 2 раза\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, 2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        self.linear3 = nn.Linear(2 * 14 * 14, output_dim)\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.bn1(self.conv1(x))))\n",
    "        x = self.do2(self.activation(self.bn2(self.conv2(x))))\n",
    "        x = self.activation(self.bn3(self.conv3(x))) # B x 2 x 14 x 14\n",
    "        \n",
    "        return self.linear3(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00e30818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = ConvModel(input_ch, hidden_ch, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a265fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.355731725692749\n",
      "epoch: 0, step: 200, loss: 0.6383442282676697\n",
      "epoch: 0, step: 400, loss: 0.38057294487953186\n",
      "epoch: 1, step: 0, loss: 0.3475794494152069\n",
      "epoch: 1, step: 200, loss: 0.1900980919599533\n",
      "epoch: 1, step: 400, loss: 0.21323086321353912\n",
      "epoch: 2, step: 0, loss: 0.16460172832012177\n",
      "epoch: 2, step: 200, loss: 0.1686076670885086\n",
      "epoch: 2, step: 400, loss: 0.06156535446643829\n",
      "epoch: 3, step: 0, loss: 0.11585879325866699\n",
      "epoch: 3, step: 200, loss: 0.10740355402231216\n",
      "epoch: 3, step: 400, loss: 0.12296571582555771\n",
      "epoch: 4, step: 0, loss: 0.14023280143737793\n",
      "epoch: 4, step: 200, loss: 0.09796636551618576\n",
      "epoch: 4, step: 400, loss: 0.15493854880332947\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn_conv,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model_conv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    torch.save(model_conv.state_dict(), f'./chkpt_conv_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Снова сигмоида хуже отработала"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547799f0",
   "metadata": {},
   "source": [
    "## Добавим слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96d9c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_ch, hidden_ch, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, hidden_ch, \n",
    "                               kernel_size=5, padding=2, stride=2) # уменьшаем картинку в 2 раза\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv4 = nn.Conv2d(hidden_ch, 2, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        \n",
    "        self.linear3 = nn.Linear(2 * 14 * 14, output_dim)\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.do3 = nn.Dropout(dropout_p)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation(self.bn1(self.conv1(x))))\n",
    "        x = self.do2(self.activation(self.bn2(self.conv2(x))))\n",
    "        x = self.do3(self.activation(self.bn3(self.conv3(x))))\n",
    "        x = self.activation(self.bn4(self.conv4(x))) # B x 2 x 14 x 14\n",
    "        \n",
    "        return self.linear3(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12b61646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = ConvModel(input_ch, hidden_ch, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70fd8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.333627462387085\n",
      "epoch: 0, step: 200, loss: 0.0845581665635109\n",
      "epoch: 0, step: 400, loss: 0.06749556213617325\n",
      "epoch: 1, step: 0, loss: 0.09035209566354752\n",
      "epoch: 1, step: 200, loss: 0.03046363964676857\n",
      "epoch: 1, step: 400, loss: 0.07565552741289139\n",
      "epoch: 2, step: 0, loss: 0.08403101563453674\n",
      "epoch: 2, step: 200, loss: 0.006629061885178089\n",
      "epoch: 2, step: 400, loss: 0.07137312740087509\n",
      "epoch: 3, step: 0, loss: 0.01628328301012516\n",
      "epoch: 3, step: 200, loss: 0.03826184570789337\n",
      "epoch: 3, step: 400, loss: 0.02981729619204998\n",
      "epoch: 4, step: 0, loss: 0.004089582245796919\n",
      "epoch: 4, step: 200, loss: 0.12291419506072998\n",
      "epoch: 4, step: 400, loss: 0.03017604537308216\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn_conv,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model_conv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    torch.save(model_conv.state_dict(), f'./chkpt_conv_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# результат стал несколько лучше, однако время исполнения сильно возросло"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6782ab1",
   "metadata": {},
   "source": [
    "### Ну и в конце попробуем применить разные активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ee3b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_ch, hidden_ch, output_dim, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, hidden_ch, \n",
    "                               kernel_size=5, padding=2, stride=2) # уменьшаем картинку в 2 раза\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_ch)\n",
    "        self.conv4 = nn.Conv2d(hidden_ch, 2, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(2)\n",
    "        \n",
    "        self.linear3 = nn.Linear(2 * 14 * 14, output_dim)\n",
    "        self.do1 = nn.Dropout(dropout_p)\n",
    "        self.do2 = nn.Dropout(dropout_p)\n",
    "        self.do3 = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do1(self.activation1(self.bn1(self.conv1(x))))\n",
    "        x = self.do2(self.activation2(self.bn2(self.conv2(x))))\n",
    "        x = self.do3(self.activation1(self.bn3(self.conv3(x))))\n",
    "        x = self.activation2(self.bn4(self.conv4(x))) # B x 2 x 14 x 14\n",
    "        \n",
    "        return self.linear3(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abb15622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = ConvModel(input_ch, hidden_ch, out_dim).to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a88154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.3511531352996826\n",
      "epoch: 0, step: 200, loss: 0.27509719133377075\n",
      "epoch: 0, step: 400, loss: 0.1266573667526245\n",
      "epoch: 1, step: 0, loss: 0.05639024078845978\n",
      "epoch: 1, step: 200, loss: 0.06566277891397476\n",
      "epoch: 1, step: 400, loss: 0.06121411547064781\n",
      "epoch: 2, step: 0, loss: 0.09947463124990463\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn_conv,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        predict = model_conv(batch['data'].to(device))\n",
    "        loss = loss_func(predict, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "    torch.save(model_conv.state_dict(), f'./chkpt_conv_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вроде существенных изменений по сравнению с прошлым шагом не наблюдается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b6aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
