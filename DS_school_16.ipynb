{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hackerrank.com/challenges/detect-the-email-addresses/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BRC1-k81pIW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "txt='\\n'.join([input() for k in range(int(input()))])\n",
    "print(*sorted(set(re.findall(r'[_0-9a-zA-Z]+[_\\.0-9a-zA-Z]*@[0-9a-zA-Z_]+[\\.0-9a-zA-Z_]+[0-9a-zA-Z_]+',txt))),sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m txt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28minput\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m()))])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://(?:ww(?:w|2)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.)?([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-]*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.[a-zA-Z]+)\u001b[39m\u001b[38;5;124m'\u001b[39m,txt))),sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\DS\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32mD:\\DS\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt='\\n'.join([input() for k in range(int(input()))])\n",
    "print(*sorted(set(re.findall(r'https?://(?:ww(?:w|2)\\.)?([\\w\\.\\-]*\\.[a-zA-Z]+)',txt))),sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать stemming, lemmatization & BoW на следующем датасете: \n",
    "https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy3\n",
      "  Obtaining dependency information for pymorphy3 from https://files.pythonhosted.org/packages/ee/53/862f7b7f3e488e5420bebd5cf59362cb175463ad3cfddd61ade15a738dc7/pymorphy3-2.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pymorphy3-2.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in d:\\ds\\anaconda\\lib\\site-packages (from pymorphy3) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
      "  Obtaining dependency information for pymorphy3-dicts-ru from https://files.pythonhosted.org/packages/b0/67/469e9e52d046863f5959928794d3067d455a77f580bf4a662630a43eb426/pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.2 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 20.5/53.2 kB 320.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 53.2/53.2 kB 550.2 kB/s eta 0:00:00\n",
      "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.4 MB 1.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/8.4 MB 3.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.6/8.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/8.4 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.2/8.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.4/8.4 MB 23.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 23.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pymorphy3-dicts-ru, pymorphy3\n",
      "Successfully installed pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142\n"
     ]
    }
   ],
   "source": [
    "! pip install pymorphy3\n",
    "! pip install pymorphy3-dicts-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\magvl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\magvl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\DS\\DS school\\labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "morph = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем первые 100 строк из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for x in data['comment'][:100]:\n",
    "    data_tok.append(tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация и стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lem_stem = []\n",
    "for i in data_tok:\n",
    "    st = []\n",
    "    for j in i:\n",
    "        st.append(snowball.stem(morph.normal_forms(j)[0]))\n",
    "    data_lem_stem.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    vector=[]\n",
    "    for w in vocab_filt:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in data_lem_stem:\n",
    "    for j in i:\n",
    "        words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальное количество слов и символов: 6130\n"
     ]
    }
   ],
   "source": [
    "print(f'Изначальное количество слов и символов: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = unique(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов и символов: 1958\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество уникальных слов и символов: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\magvl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filt = []\n",
    "for w in vocab: \n",
    "    if w not in stop_words and w not in special_char: \n",
    "        vocab_filt.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов и символов без стоп-слов и символов: 1895\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество уникальных слов и символов без стоп-слов и символов: {len(vocab_filt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество векторов: 100\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i in data_lem_stem:\n",
    "    vectors.append(vectorize(i))\n",
    "print(f\"Количество векторов: {len(vectors)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наиболее схожие строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее схожие строки:\n",
      "['взят', 'как', 'пример', 'росс', ',', 'западноевропейск', 'стран', 'и', 'сша', '.', 'идт', 'метисац', ',', 'сознательн', 'политик', 'замещен', 'бел', 'населен', 'на', 'пришл', 'черн', '-', 'коричнев', '.', 'идт', 'создан', 'нов', 'рас', 'метис', ',', 'исламизац', 'и', 'почернен', '.', 'в', 'крупн', 'город', 'половин', 'населен', '-', 'выходец', 'из', 'ебен', 'мексик', ',', 'африк', ',', 'ближн', 'восток', ',', 'а', 'в', 'случа', 'с', 'росс', '-', 'кавказ', 'и', 'средн', 'аз', '.', 'этническ', 'ниггер', '-', 'арабск', 'гетт', 'вер', 'на', 'ху', 'закон', 'как', 'хотет', ',', 'чудовищн', 'по', 'масштаб', 'этническ', 'преступн', '.', 'говор', 'о', 'миграц', 'и', 'тем', 'бол', 'затрагива', 'тем', 'замещен', 'корен', 'населен', 'власт', 'нельз', ',', 'инач', 'бутылк', '.', 'свобод', 'слов', 'тут', 'не', 'для', 'вы', ',', 'молод', 'человек', '.', 'при', 'эт', 'говор', 'о', 'тот', ',', 'что', 'бел', 'должн', 'вымерет', ',', 'и', 'эт', 'нормальн', '-', 'можн', '.', 'бел', 'официальн', 'вымира', 'вест', 'пропаганд', 'так', 'ил', 'инач', 'направ', 'на', 'снижен', 'рождаем', 'бел', 'населен', '.', 'феминизм', ',', 'лгбт', ',', 'чайлдфр', '.', 'кажд', 'женщин', 'в', 'швец', '-', 'леволиберальн', 'феминистк', ',', 'эт', 'стран', 'побед', 'феминизм', '.', 'что', 'сегодн', 'там', 'происход', '-', 'страшн', 'дела', '.', 'пропагандирова', 'смеша', 'брак', ',', 'межрасов', 'брак', ',', 'пропагандирова', 'превосходств', 'ребенок', '-', 'метис', '.', 'идт', 'демонизац', 'бел', 'и', 'пропаганд', 'превосходств', 'черн', 'и', 'смугл', 'мужчин', ',', 'форс', 'отношен', 'бел', 'женщин', 'смугл', 'черн', 'мужчин', '-', 'мигрант', '.', 'как', 'результат', '-', 'все', 'больш', 'чернильниц', ',', 'все', 'больш', 'смеша', 'брак', ',', 'все', 'больш', 'небел', 'метис', '.', 'бел', 'женщин', 'прост', 'не', 'хотет', 'контактирова', 'с', 'мужчин', 'сво', 'нац', 'и', 'рас', ',', 'наделя', 'он', 'сам', 'плох', 'качеств', 'и', 'обожествля', 'черн', '.', 'при', 'эт', 'большинств', 'бел', 'не', 'счита', 'завоз', 'чурк', 'чем', '-', 'то', 'плох', ',', 'наоборот', ',', 'относ', 'к', 'он', 'толерантн', '.', 'провод', 'политик', 'насажден', 'толерантн', ',', 'мультикультурализм', ',', 'политкорректн', 'и', 'космополитизм', '.', 'набира', 'популярн', 'даж', 'в', 'росс', 'sjw', '-', 'эт', 'вообщ', 'отдельн', 'тем', 'для', 'обсужден', '.', 'все', 'вышеперечислен', 'относ', 'к', 'сильн', 'когд', '-', 'то', 'стран', ',', 'бывш', 'импер', ',', 'нагиба', 'слаб', '.', 'сегодн', 'происход', 'так', ',', 'что', 'бывш', 'импер', 'в', 'прям', 'смысл', 'деградирова', ',', 'вырожда', 'и', 'вымира', ',', 'а', 'мест', 'сильн', 'когд', '-', 'то', ',', 'господств', 'народ', ',', 'занима', 'тот', ',', 'кто', 'когд', '-', 'то', 'колонизирова', '.', 'в', 'франц', 'к', '2080', 'уж', 'быт', 'доминирова', 'негр', 'и', 'араб', ',', 'в', 'росс', '-', 'кавказец', 'и', 'выходец', 'из', 'средн', 'аз', ',', 'в', 'великобритан', '-', 'индиец', ',', 'негр', ',', 'араб', ',', 'пакистанец', ',', 'etc', '.', 'а', 'в', 'маленьк', ',', 'нейтральн', 'стран', ',', 'врод', 'словен', 'ил', 'белар', ',', 'литв', 'ил', 'чех', ',', 'румын', 'ил', 'эстон', '-', 'все', 'пучок', '.', 'он', 'вымиран', 'не', 'гроз', ',', 'он', 'остава', 'и', 'быт', 'остава', 'бел', '.', 'бол', 'тот', ',', 'у', 'он', 'вест', 'политик', ',', 'направ', 'на', 'сохранен', 'традицион', 'ценност', 'и', 'культур', 'корен', 'населен', '.', 'он', 'сказа', 'беженец', 'нет', '.', 'в', 'польш', ',', 'например', ',', 'русск', 'ил', 'украинец', 'горазд', 'легк', 'перееха', 'и', 'оста', ',', 'чем', 'араб', 'ил', 'африканец', '.', 'в', 'герман', 'ситуац', 'противоположн', ',', 'бел', 'там', 'не', 'ждат', '.', 'польш', ',', 'чех', ',', 'словак', ',', 'венгр', ',', 'словен', ',', 'хорват', ',', 'серб', ',', 'биг', ',', 'черногор', ',', 'македон', ',', 'грец', ',', 'болгар', ',', 'румын', ',', 'молдов', ',', 'украин', ',', 'белар', ',', 'литв', ',', 'латв', ',', 'эстон', '-', 'вот', 'европ', 'будущ', '.', 'скандинав', ',', 'южн', ',', 'западн', 'европ', ',', 'а', 'такж', 'росс', '-', 'лиш', 'корен', 'населен', 'и', 'сво', 'культур', '.']\n",
      "['туп', '-', 'туп', '-', 'туп', ',', 'ти', '-', 'ля', '-', 'ля', '-', 'лю', '-', 'лю', '!']\n",
      "Косинусная мера между строками: 0.49863839467117715\n"
     ]
    }
   ],
   "source": [
    "matrix = []\n",
    "for a in vectors:\n",
    "    string = []\n",
    "    for b in vectors:\n",
    "        string.append(dot(a, b)/(norm(a)*norm(b)))\n",
    "    matrix.append(string)\n",
    "df = pd.DataFrame(matrix)\n",
    "for i in range(df.shape[0]):\n",
    "    df.iat[i,i] = 0\n",
    "str = df.max().idxmax()\n",
    "col = df.loc[str,:].idxmax()\n",
    "print(\"Наиболее схожие строки:\")\n",
    "print(data_lem_stem[str])\n",
    "print(data_lem_stem[col])\n",
    "print(f\"Косинусная мера между строками: {df.max().max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат по поиску схожих строк выглядит не логичным. Возможно, я где-то ошибся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
